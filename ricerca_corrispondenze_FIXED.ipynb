{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e547b931-8a53-47c8-97c3-c54d453cac8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PASS1 1/1000] n_scheda=41125\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# Biblio longform -> enrich (OPAC SBN -> Open Library -> LoC -> Internet Archive)\n",
    "# TWO-PASS MODE:\n",
    "#   Pass 1: standard cascade, strict threshold\n",
    "#   Pass 2: only NOT_FOUND, more query variants + relaxed threshold\n",
    "# Output: LONG enriched + WIDE enriched\n",
    "# Key requirement: ext_url ALWAYS \"human record page\" (incl. OPAC SBN)\n",
    "# ===========================\n",
    "\n",
    "import os, re, json, time\n",
    "from difflib import SequenceMatcher\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# ---------------------------\n",
    "# CONFIG\n",
    "# ---------------------------\n",
    "INPUT_CSV     = \"biblio_longform_all_records.csv\"\n",
    "OUTPUT_LONG   = \"biblio_longform_all_records_enriched.csv\"\n",
    "OUTPUT_WIDE   = \"biblio_wide_records_enriched.csv\"\n",
    "PROGRESS_JSON = \"biblio_enrich_progress.json\"\n",
    "\n",
    "TOPK_PER_SOURCE = 12\n",
    "TIMEOUT = 30\n",
    "SLEEP_BETWEEN_CALLS = 0.25\n",
    "\n",
    "MIN_SCORE_KEEP = 0.12\n",
    "MIN_SCORE_FOUND_1ST = 0.40     # first pass acceptance\n",
    "MIN_SCORE_FOUND_2ND = 0.28     # second pass acceptance (ONLY for NOT_FOUND)\n",
    "\n",
    "ALLOW_AUTHOR_ONLY_QUERIES = True\n",
    "DEBUG_SBN = False\n",
    "\n",
    "CONTACT_EMAIL = \"pietro.terna@unito.it\"\n",
    "UA = f\"biblio-enricher/5.0 (Jupyter; contact: {CONTACT_EMAIL})\"\n",
    "\n",
    "STANDARD_FIELDS = [\n",
    "    \"ext_source\",\"ext_match_score\",\"ext_title\",\"ext_author\",\"ext_year\",\n",
    "    \"ext_publisher\",\"ext_place\",\"ext_language\",\"ext_physical_desc\",\n",
    "    \"ext_identifiers\",\"ext_notes\",\"ext_url\"\n",
    "]\n",
    "\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\"User-Agent\": UA})\n",
    "\n",
    "def _sleep():\n",
    "    time.sleep(SLEEP_BETWEEN_CALLS)\n",
    "\n",
    "# ---------------------------\n",
    "# Helpers\n",
    "# ---------------------------\n",
    "def norm(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = str(s).strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = re.sub(r\"[“”\\\"'’`]\", \"\", s)\n",
    "    s = re.sub(r\"[^0-9a-zàèéìòùçäëïöüßñ \\-:/]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def sim(a: str, b: str) -> float:\n",
    "    a, b = norm(a), norm(b)\n",
    "    if not a or not b:\n",
    "        return 0.0\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def extract_year(s: str):\n",
    "    if not s:\n",
    "        return None\n",
    "    m = re.search(r\"(1[5-9]\\d{2}|20\\d{2})\", str(s))\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def normalize_year_from_data_pub(data_pub: str):\n",
    "    return extract_year(data_pub)\n",
    "\n",
    "def parse_series(collezion: str):\n",
    "    if not collezion:\n",
    "        return (\"\", \"\")\n",
    "    s = str(collezion).strip()\n",
    "    m = re.search(r\"\\bN\\.?\\s*(\\d+)\\b\", s, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        no = m.group(1)\n",
    "        series = re.sub(r\"\\bN\\.?\\s*\\d+\\b\", \"\", s, flags=re.IGNORECASE).strip(\" ,;-\")\n",
    "        return (series.strip(), no)\n",
    "    return (s, \"\")\n",
    "\n",
    "def pack_identifiers(d: dict) -> str:\n",
    "    return json.dumps(d or {}, ensure_ascii=False)\n",
    "\n",
    "def safe_list(v):\n",
    "    if v is None:\n",
    "        return []\n",
    "    if isinstance(v, list):\n",
    "        return v\n",
    "    return [v]\n",
    "\n",
    "def first_nonempty(vals):\n",
    "    for v in vals:\n",
    "        if v is None:\n",
    "            continue\n",
    "        s = str(v).strip()\n",
    "        if s and s.lower() != \"nan\":\n",
    "            return s\n",
    "    return \"\"\n",
    "\n",
    "def load_progress():\n",
    "    if os.path.exists(PROGRESS_JSON):\n",
    "        with open(PROGRESS_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    return {\"done_n_scheda\": [], \"last_book_number\": 0}\n",
    "\n",
    "def save_progress(prog):\n",
    "    with open(PROGRESS_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(prog, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# ---------------------------\n",
    "# OPAC SBN \"page url\"\n",
    "# ---------------------------\n",
    "def sbn_bid_to_page_url(bid: str) -> str:\n",
    "    if not isinstance(bid, str) or not bid.strip():\n",
    "        return \"\"\n",
    "    bid = bid.strip()\n",
    "    m = re.match(r\"^IT\\\\ICCU\\\\([A-Za-z0-9]{3})\\\\([0-9]{1,10})$\", bid)\n",
    "    if m:\n",
    "        polo = m.group(1).upper()\n",
    "        num = m.group(2).zfill(7)\n",
    "        return f\"https://opac.sbn.it/bid/{polo}{num}\"\n",
    "    bid_clean = bid.replace(\"\\\\\", \"\")\n",
    "    return f\"https://opac.sbn.it/bid/{bid_clean}\"\n",
    "\n",
    "# ---------------------------\n",
    "# SBN helpers\n",
    "# ---------------------------\n",
    "def sbn_clean_query(s: str) -> str:\n",
    "    s = (s or \"\")\n",
    "    s = re.sub(r\"[\\\"'’`]\", \"\", s)\n",
    "    s = re.sub(r\"[,.;:()\\\\[\\\\]{}<>|]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def sbn_clean_author(a: str) -> str:\n",
    "    # remove common honorifics/titles and trailing qualifiers\n",
    "    a = (a or \"\")\n",
    "    a = re.sub(r\"\\b(card\\.?|arciv\\.?|mons\\.?|rev\\.?|s\\.j\\.?|o\\.p\\.?)\\b\", \" \", a, flags=re.IGNORECASE)\n",
    "    a = re.sub(r\"\\s+\", \" \", a).strip()\n",
    "    return a\n",
    "\n",
    "def sbn_clean_title(t: str) -> str:\n",
    "    t = (t or \"\")\n",
    "    t = re.sub(r\"\\s*/\\s*.*$\", \"\", t)  # drop anything after \" / \" (often author)\n",
    "    t = re.sub(r\"\\s*:\\s*.*$\", \"\", t)  # optionally shorten after \":\" for very long titles\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "def sbn_extract_hits(js):\n",
    "    if not isinstance(js, dict):\n",
    "        return []\n",
    "    for k in [\"briefRecords\", \"records\", \"record\", \"results\", \"elenco\", \"items\", \"docs\"]:\n",
    "        v = js.get(k)\n",
    "        if isinstance(v, list):\n",
    "            return v\n",
    "    for k in [\"result\", \"response\", \"data\"]:\n",
    "        v = js.get(k)\n",
    "        if isinstance(v, dict):\n",
    "            for k2 in [\"briefRecords\", \"records\", \"items\", \"docs\", \"results\", \"elenco\"]:\n",
    "                v2 = v.get(k2)\n",
    "                if isinstance(v2, list):\n",
    "                    return v2\n",
    "    return []\n",
    "\n",
    "def series_bonus(book, text):\n",
    "    series = norm(book.get(\"series\",\"\"))\n",
    "    s_no   = norm(book.get(\"series_no\",\"\"))\n",
    "    t = norm(text)\n",
    "    bonus = 0.0\n",
    "    if series and series in t:\n",
    "        bonus += 0.10\n",
    "    if s_no and re.search(rf\"\\b{s_no}\\b\", t):\n",
    "        bonus += 0.06\n",
    "    return bonus\n",
    "\n",
    "# ---------------------------\n",
    "# Load input\n",
    "# ---------------------------\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# Support both historical longform layouts:\n",
    "#   OLD: columns = n_scheda, campo, valore\n",
    "#   NEW: columns = field, value, with \"n_scheda\" emitted as a row that starts each record\n",
    "if {\"n_scheda\",\"campo\",\"valore\"}.issubset(df.columns):\n",
    "    df[\"n_scheda\"] = pd.to_numeric(df[\"n_scheda\"], errors=\"coerce\")\n",
    "    if df[\"n_scheda\"].isna().any():\n",
    "        bad = df.loc[df[\"n_scheda\"].isna()].head(10)\n",
    "        raise ValueError(f\"n_scheda non numerico in alcune righe (prime 10):\\n{bad}\")\n",
    "    df[\"n_scheda\"] = df[\"n_scheda\"].astype(int)\n",
    "    df[\"campo\"] = df[\"campo\"].astype(str)\n",
    "    df[\"valore\"] = df[\"valore\"].astype(str)\n",
    "\n",
    "elif {\"field\",\"value\"}.issubset(df.columns):\n",
    "    # Normalize column names used downstream\n",
    "    df[\"campo\"] = df[\"field\"].astype(str)\n",
    "    df[\"valore\"] = df[\"value\"].astype(str)\n",
    "\n",
    "    # Reconstruct n_scheda if it is encoded as a \"field\" row\n",
    "    if \"n_scheda\" not in df.columns:\n",
    "        mask_ns = df[\"campo\"].str.strip().str.lower().eq(\"n_scheda\")\n",
    "        if not mask_ns.any():\n",
    "            raise ValueError(\n",
    "                \"CSV nel formato field/value ma senza colonna n_scheda e senza righe con field='n_scheda'. \"\n",
    "                f\"Colonne trovate={list(df.columns)}\"\n",
    "            )\n",
    "        df[\"n_scheda\"] = pd.to_numeric(df[\"valore\"].where(mask_ns), errors=\"coerce\").ffill()\n",
    "        if df[\"n_scheda\"].isna().any():\n",
    "            bad = df.loc[df[\"n_scheda\"].isna()].head(10)\n",
    "            raise ValueError(f\"Impossibile ricostruire n_scheda (prime 10 righe problematiche):\\n{bad}\")\n",
    "        df[\"n_scheda\"] = df[\"n_scheda\"].astype(int)\n",
    "        # Drop the structural rows that only carry n_scheda\n",
    "        df = df.loc[~mask_ns].copy()\n",
    "    else:\n",
    "        df[\"n_scheda\"] = pd.to_numeric(df[\"n_scheda\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        if df[\"n_scheda\"].isna().any():\n",
    "            bad = df.loc[df[\"n_scheda\"].isna()].head(10)\n",
    "            raise ValueError(f\"n_scheda non numerico in alcune righe (prime 10):\\n{bad}\")\n",
    "        df[\"n_scheda\"] = df[\"n_scheda\"].astype(int)\n",
    "\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"CSV non nel formato atteso. Colonne trovate={list(df.columns)}; \"\n",
    "        \"attesi (n_scheda,campo,valore) oppure (field,value) con righe n_scheda.\"\n",
    "    )\n",
    "\n",
    "groups = list(df.groupby(\"n_scheda\", sort=True))\n",
    "n_books = len(groups)\n",
    "\n",
    "def book_input_from_group(g: pd.DataFrame) -> dict:\n",
    "    rec = {\"n_scheda\": int(g[\"n_scheda\"].iloc[0])}\n",
    "\n",
    "    def fv(field):\n",
    "        vals = g.loc[g[\"campo\"] == field, \"valore\"].tolist()\n",
    "        return first_nonempty(vals)\n",
    "\n",
    "    rec[\"autore\"]    = fv(\"autore\")\n",
    "    rec[\"titolo\"]    = fv(\"titolo\")\n",
    "    rec[\"editore\"]   = fv(\"editore\")\n",
    "    rec[\"luogo_pub\"] = fv(\"luogo_pub\")\n",
    "    rec[\"data_pub\"]  = fv(\"data_pub\")\n",
    "    rec[\"cod_isbn\"]  = fv(\"cod_isbn\")\n",
    "    rec[\"collezion\"] = fv(\"collezion\")\n",
    "\n",
    "    rec[\"year\"] = normalize_year_from_data_pub(rec[\"data_pub\"]) or extract_year(rec[\"titolo\"])\n",
    "    series, series_no = parse_series(rec[\"collezion\"])\n",
    "    rec[\"series\"] = series\n",
    "    rec[\"series_no\"] = series_no\n",
    "    return rec\n",
    "\n",
    "def build_queries_pass1(book: dict):\n",
    "    titolo  = (book.get(\"titolo\") or \"\").strip()\n",
    "    autore  = (book.get(\"autore\") or \"\").strip()\n",
    "    editore = (book.get(\"editore\") or \"\").strip()\n",
    "    year    = book.get(\"year\")\n",
    "    series  = (book.get(\"series\") or \"\").strip()\n",
    "    s_no    = (book.get(\"series_no\") or \"\").strip()\n",
    "\n",
    "    queries = []\n",
    "    if titolo:\n",
    "        q1 = \" \".join([x for x in [titolo, autore, str(year) if year else \"\"] if x]).strip()\n",
    "        q2 = \" \".join([x for x in [titolo, autore] if x]).strip()\n",
    "        q3 = \" \".join([x for x in [titolo, autore, editore, str(year) if year else \"\"] if x]).strip()\n",
    "        for q in [q1, q2, q3]:\n",
    "            if q and q not in queries:\n",
    "                queries.append(q)\n",
    "        return queries\n",
    "\n",
    "    # no title\n",
    "    qA = \" \".join([x for x in [autore, series, s_no, editore, str(year) if year else \"\"] if x]).strip()\n",
    "    qB = \" \".join([x for x in [autore, series, s_no] if x]).strip()\n",
    "    qC = \" \".join([x for x in [autore, series] if x]).strip()\n",
    "    qD = \" \".join([x for x in [autore, str(year) if year else \"\", editore] if x]).strip()\n",
    "\n",
    "    for q in [qA, qB, qC, qD]:\n",
    "        if q and q not in queries:\n",
    "            queries.append(q)\n",
    "\n",
    "    if ALLOW_AUTHOR_ONLY_QUERIES and autore and autore not in queries:\n",
    "        queries.append(autore)\n",
    "\n",
    "    return queries\n",
    "\n",
    "def build_queries_pass2(book: dict):\n",
    "    \"\"\"\n",
    "    More aggressive variants (only used for NOT_FOUND).\n",
    "    \"\"\"\n",
    "    titolo  = (book.get(\"titolo\") or \"\").strip()\n",
    "    autore  = (book.get(\"autore\") or \"\").strip()\n",
    "    editore = (book.get(\"editore\") or \"\").strip()\n",
    "    luogo   = (book.get(\"luogo_pub\") or \"\").strip()\n",
    "    year    = book.get(\"year\")\n",
    "    series  = (book.get(\"series\") or \"\").strip()\n",
    "    s_no    = (book.get(\"series_no\") or \"\").strip()\n",
    "\n",
    "    autore2 = sbn_clean_author(autore)\n",
    "    titolo2 = sbn_clean_title(titolo)\n",
    "\n",
    "    variants = []\n",
    "\n",
    "    # title present: include cleaned title/author variants, and drop publisher/year variants too\n",
    "    if titolo:\n",
    "        base = [\n",
    "            \" \".join([x for x in [titolo, autore] if x]),\n",
    "            \" \".join([x for x in [titolo2, autore] if x]),\n",
    "            \" \".join([x for x in [titolo2, autore2] if x]),\n",
    "            \" \".join([x for x in [titolo2, autore2, str(year) if year else \"\"] if x]),\n",
    "            \" \".join([x for x in [titolo2, editore] if x]),\n",
    "            \" \".join([x for x in [titolo2, luogo] if x]),\n",
    "        ]\n",
    "        for q in base:\n",
    "            q = q.strip()\n",
    "            if q and q not in variants:\n",
    "                variants.append(q)\n",
    "        # author-only as last resort\n",
    "        if ALLOW_AUTHOR_ONLY_QUERIES and autore2 and autore2 not in variants:\n",
    "            variants.append(autore2)\n",
    "        return variants\n",
    "\n",
    "    # no title: lean on series / number / publisher / place\n",
    "    base = [\n",
    "        \" \".join([x for x in [autore2, series, s_no] if x]),\n",
    "        \" \".join([x for x in [series, s_no, editore] if x]),\n",
    "        \" \".join([x for x in [autore2, editore, luogo] if x]),\n",
    "        \" \".join([x for x in [autore2, str(year) if year else \"\", editore] if x]),\n",
    "        \" \".join([x for x in [series, editore] if x]),\n",
    "        \" \".join([x for x in [autore2] if x]),\n",
    "    ]\n",
    "    for q in base:\n",
    "        q = q.strip()\n",
    "        if q and q not in variants:\n",
    "            variants.append(q)\n",
    "    return variants\n",
    "\n",
    "# ---------------------------\n",
    "# OPAC SBN API\n",
    "# ---------------------------\n",
    "def sbn_search_any(q: str, rows=TOPK_PER_SOURCE):\n",
    "    url = f\"https://opac.sbn.it/opacmobilegw/search.json?any={quote_plus(q)}&type=0&start=0&rows={rows}\"\n",
    "    r = SESSION.get(url, timeout=TIMEOUT)\n",
    "    _sleep()\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def sbn_search_isbn(isbn: str, rows=TOPK_PER_SOURCE):\n",
    "    url = f\"https://opac.sbn.it/opacmobilegw/search.json?isbn={quote_plus(isbn)}&start=0&rows={rows}\"\n",
    "    r = SESSION.get(url, timeout=TIMEOUT)\n",
    "    _sleep()\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def sbn_full(bid: str):\n",
    "    url = f\"https://opac.sbn.it/opacmobilegw/full.json?bid={quote_plus(bid)}\"\n",
    "    r = SESSION.get(url, timeout=TIMEOUT)\n",
    "    _sleep()\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def _score_sbn_hits(book, hits):\n",
    "    titolo_in = (book.get(\"titolo\") or \"\").strip()\n",
    "    autore_in = sbn_clean_author(book.get(\"autore\") or \"\").strip()\n",
    "    year_in   = book.get(\"year\")\n",
    "\n",
    "    cands = []\n",
    "    for h in (hits or [])[:TOPK_PER_SOURCE]:\n",
    "        title = h.get(\"titolo\") or \"\"\n",
    "        auth  = h.get(\"autorePrincipale\") or \"\"\n",
    "        pub   = h.get(\"pubblicazione\") or \"\"\n",
    "        bid   = h.get(\"codiceIdentificativo\") or \"\"\n",
    "        by = extract_year(pub)\n",
    "\n",
    "        if titolo_in:\n",
    "            score = 0.70 * sim(titolo_in, title) + 0.30 * sim(autore_in, auth)\n",
    "        else:\n",
    "            score = 0.85 * sim(autore_in, auth) + 0.15 * (0.2 if title else 0.0)\n",
    "\n",
    "        score += series_bonus(book, f\"{title} {pub}\")\n",
    "\n",
    "        if year_in and by and abs(year_in - by) <= 1:\n",
    "            score += 0.06\n",
    "\n",
    "        if score >= MIN_SCORE_KEEP:\n",
    "            cands.append({\"source\":\"OPAC SBN\",\"score\":score,\"title\":title,\"author\":auth,\"pub\":pub,\"bid\":bid,\"raw\":h})\n",
    "\n",
    "    cands.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "    for c in cands[:2]:\n",
    "        if c.get(\"bid\"):\n",
    "            try:\n",
    "                c[\"full\"] = sbn_full(c[\"bid\"])\n",
    "            except Exception:\n",
    "                c[\"full\"] = None\n",
    "    return cands\n",
    "\n",
    "def sbn_candidates(book: dict, pass_no: int):\n",
    "    isbn = (book.get(\"cod_isbn\") or \"\").strip()\n",
    "\n",
    "    queries = build_queries_pass1(book) if pass_no == 1 else build_queries_pass2(book)\n",
    "\n",
    "    # ISBN-first in both passes\n",
    "    if isbn:\n",
    "        try:\n",
    "            js = sbn_search_isbn(isbn)\n",
    "            hits = sbn_extract_hits(js)\n",
    "            if DEBUG_SBN:\n",
    "                print(f\"   SBN[isbn] numFound={js.get('numFound')} hits={len(hits)}\")\n",
    "            c = _score_sbn_hits(book, hits)\n",
    "            if c:\n",
    "                return c\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # any= variants\n",
    "    for q in queries:\n",
    "        if not q:\n",
    "            continue\n",
    "        for qv in [q, sbn_clean_query(q)]:\n",
    "            if not qv:\n",
    "                continue\n",
    "            try:\n",
    "                js = sbn_search_any(qv)\n",
    "                hits = sbn_extract_hits(js)\n",
    "                if DEBUG_SBN:\n",
    "                    print(f\"   SBN[any] hits={len(hits)} q='{qv[:70]}'\")\n",
    "                c = _score_sbn_hits(book, hits)\n",
    "                if c:\n",
    "                    return c\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    return []\n",
    "\n",
    "# ---------------------------\n",
    "# Open Library\n",
    "# ---------------------------\n",
    "def ol_search_q(q: str, rows=TOPK_PER_SOURCE):\n",
    "    params = {\"q\": q, \"limit\": rows}\n",
    "    r = SESSION.get(\"https://openlibrary.org/search.json\", params=params, timeout=TIMEOUT)\n",
    "    _sleep()\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def ol_candidates(book: dict, pass_no: int):\n",
    "    isbn = (book.get(\"cod_isbn\") or \"\").strip()\n",
    "    queries = []\n",
    "    if isbn:\n",
    "        queries.append(f\"isbn:{isbn}\")\n",
    "    queries.extend(build_queries_pass1(book) if pass_no == 1 else build_queries_pass2(book))\n",
    "\n",
    "    titolo_in = (book.get(\"titolo\") or \"\").strip()\n",
    "    autore_in = sbn_clean_author(book.get(\"autore\") or \"\").strip()\n",
    "    year_in   = book.get(\"year\")\n",
    "\n",
    "    for q in queries:\n",
    "        if not q:\n",
    "            continue\n",
    "        try:\n",
    "            js = ol_search_q(q)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        docs = js.get(\"docs\", [])[:TOPK_PER_SOURCE]\n",
    "        cands = []\n",
    "        for d in docs:\n",
    "            title = d.get(\"title\") or \"\"\n",
    "            auth  = (d.get(\"author_name\") or [\"\"])[0]\n",
    "            year  = d.get(\"first_publish_year\")\n",
    "            pub   = (d.get(\"publisher\") or [\"\"])[0] if isinstance(d.get(\"publisher\"), list) else (d.get(\"publisher\") or \"\")\n",
    "            place = (d.get(\"publish_place\") or [\"\"])[0] if isinstance(d.get(\"publish_place\"), list) else (d.get(\"publish_place\") or \"\")\n",
    "            lang  = (d.get(\"language\") or [\"\"])[0] if isinstance(d.get(\"language\"), list) else (d.get(\"language\") or \"\")\n",
    "            key   = d.get(\"key\",\"\")\n",
    "\n",
    "            if titolo_in:\n",
    "                score = 0.70 * sim(titolo_in, title) + 0.30 * sim(autore_in, auth)\n",
    "            else:\n",
    "                score = 0.85 * sim(autore_in, auth) + 0.15 * (0.2 if title else 0.0)\n",
    "\n",
    "            score += series_bonus(book, f\"{title} {pub} {place}\")\n",
    "\n",
    "            if year_in and year and abs(year_in - int(year)) <= 1:\n",
    "                score += 0.05\n",
    "\n",
    "            if score >= MIN_SCORE_KEEP:\n",
    "                cands.append({\"source\":\"Open Library\",\"score\":score,\"title\":title,\"author\":auth,\"year\":year,\n",
    "                              \"publisher\":pub,\"place\":place,\"language\":lang,\"key\":key,\"raw\":d})\n",
    "\n",
    "        cands.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "        if cands:\n",
    "            return cands\n",
    "\n",
    "    return []\n",
    "\n",
    "# ---------------------------\n",
    "# Library of Congress\n",
    "# ---------------------------\n",
    "def loc_search(q: str, rows=TOPK_PER_SOURCE):\n",
    "    params = {\"fo\":\"json\", \"q\": q, \"c\": rows}\n",
    "    r = SESSION.get(\"https://www.loc.gov/books/\", params=params, timeout=TIMEOUT)\n",
    "    _sleep()\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def loc_candidates(book: dict, pass_no: int):\n",
    "    queries = build_queries_pass1(book) if pass_no == 1 else build_queries_pass2(book)\n",
    "    titolo_in = (book.get(\"titolo\") or \"\").strip()\n",
    "    autore_in = sbn_clean_author(book.get(\"autore\") or \"\").strip()\n",
    "    year_in   = book.get(\"year\")\n",
    "\n",
    "    for q in queries:\n",
    "        if not q:\n",
    "            continue\n",
    "        try:\n",
    "            js = loc_search(q)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        results = js.get(\"results\", [])[:TOPK_PER_SOURCE]\n",
    "        cands = []\n",
    "        for it in results:\n",
    "            title = it.get(\"title\") or \"\"\n",
    "            auth_list = it.get(\"contributor\") or it.get(\"creator\") or []\n",
    "            auth = auth_list[0] if isinstance(auth_list, list) and auth_list else \"\"\n",
    "            date = it.get(\"date\") or \"\"\n",
    "            year = extract_year(date)\n",
    "            loc_id = it.get(\"id\") or \"\"\n",
    "\n",
    "            if titolo_in:\n",
    "                score = 0.70 * sim(titolo_in, title) + 0.30 * sim(autore_in, auth)\n",
    "            else:\n",
    "                score = 0.85 * sim(autore_in, auth) + 0.15 * (0.2 if title else 0.0)\n",
    "\n",
    "            score += series_bonus(book, f\"{title} {date}\")\n",
    "\n",
    "            if year_in and year and abs(year_in - year) <= 1:\n",
    "                score += 0.05\n",
    "\n",
    "            if score >= MIN_SCORE_KEEP:\n",
    "                cands.append({\"source\":\"Library of Congress\",\"score\":score,\"title\":title,\"author\":auth,\"date\":date,\"id\":loc_id,\"raw\":it})\n",
    "\n",
    "        cands.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "        if cands:\n",
    "            return cands\n",
    "\n",
    "    return []\n",
    "\n",
    "# ---------------------------\n",
    "# Internet Archive\n",
    "# ---------------------------\n",
    "def ia_advanced_search(title: str, author: str, q_extra: str = \"\", rows=TOPK_PER_SOURCE):\n",
    "    q = []\n",
    "    if title:\n",
    "        q.append(f'title:(\"{title}\")')\n",
    "    if author:\n",
    "        q.append(f'creator:(\"{author}\")')\n",
    "    if q_extra:\n",
    "        q.append(q_extra)\n",
    "    q.append('(mediatype:texts OR collection:opensource_texts)')\n",
    "    query = \" AND \".join([x for x in q if x])\n",
    "\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"fl[]\": [\"identifier\", \"title\", \"creator\", \"date\", \"publisher\", \"language\"],\n",
    "        \"rows\": rows,\n",
    "        \"page\": 1,\n",
    "        \"output\": \"json\"\n",
    "    }\n",
    "    r = SESSION.get(\"https://archive.org/advancedsearch.php\", params=params, timeout=TIMEOUT)\n",
    "    _sleep()\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def ia_metadata(identifier: str):\n",
    "    r = SESSION.get(f\"https://archive.org/metadata/{identifier}\", timeout=TIMEOUT)\n",
    "    _sleep()\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def ia_candidates(book: dict, pass_no: int):\n",
    "    titolo_in = (book.get(\"titolo\") or \"\").strip()\n",
    "    autore_in = sbn_clean_author(book.get(\"autore\") or \"\").strip()\n",
    "    year_in   = book.get(\"year\")\n",
    "    queries = build_queries_pass1(book) if pass_no == 1 else build_queries_pass2(book)\n",
    "\n",
    "    cands_all = []\n",
    "    for q in queries[:5 if pass_no==2 else 3]:\n",
    "        try:\n",
    "            js = ia_advanced_search(titolo_in if titolo_in else \"\", autore_in if autore_in else \"\", q_extra=q)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        docs = (js.get(\"response\", {}).get(\"docs\", []) or [])[:TOPK_PER_SOURCE]\n",
    "        for d in docs:\n",
    "            title = d.get(\"title\") or \"\"\n",
    "            creator = d.get(\"creator\")\n",
    "            auth = creator[0] if isinstance(creator, list) and creator else (creator or \"\")\n",
    "            year = extract_year(d.get(\"date\"))\n",
    "            ident = d.get(\"identifier\")\n",
    "\n",
    "            if titolo_in:\n",
    "                score = 0.70 * sim(titolo_in, title) + 0.30 * sim(autore_in, auth)\n",
    "            else:\n",
    "                score = 0.85 * sim(autore_in, auth) + 0.15 * (0.2 if title else 0.0)\n",
    "\n",
    "            score += series_bonus(book, f\"{title} {d.get('publisher','')}\")\n",
    "\n",
    "            if year_in and year and abs(year_in - year) <= 1:\n",
    "                score += 0.05\n",
    "\n",
    "            if score >= MIN_SCORE_KEEP:\n",
    "                cands_all.append({\"source\":\"Internet Archive\",\"score\":score,\"title\":title,\"author\":auth,\"year\":year,\n",
    "                                  \"identifier\":ident,\"raw\":d})\n",
    "\n",
    "        if cands_all:\n",
    "            break\n",
    "\n",
    "    cands_all.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "    if cands_all and cands_all[0].get(\"identifier\"):\n",
    "        try:\n",
    "            cands_all[0][\"full\"] = ia_metadata(cands_all[0][\"identifier\"])\n",
    "        except Exception:\n",
    "            cands_all[0][\"full\"] = None\n",
    "\n",
    "    return cands_all\n",
    "\n",
    "# ---------------------------\n",
    "# Map to ext_* (ext_url always human page)\n",
    "# ---------------------------\n",
    "def from_sbn(best: dict) -> dict:\n",
    "    full = best.get(\"full\") or {}\n",
    "    bid = best.get(\"bid\",\"\")\n",
    "    return {\n",
    "        \"ext_source\": \"OPAC SBN\",\n",
    "        \"ext_match_score\": f\"{best.get('score',0):.3f}\",\n",
    "        \"ext_title\": full.get(\"titolo\") or best.get(\"title\") or \"\",\n",
    "        \"ext_author\": full.get(\"autorePrincipale\") or best.get(\"author\") or \"\",\n",
    "        \"ext_year\": str(extract_year(full.get(\"pubblicazione\")) or extract_year(best.get(\"pub\")) or \"\" or \"\"),\n",
    "        \"ext_publisher\": \"\",\n",
    "        \"ext_place\": \"\",\n",
    "        \"ext_language\": full.get(\"linguaPubblicazione\") or \"\",\n",
    "        \"ext_physical_desc\": full.get(\"descrizioneFisica\") or \"\",\n",
    "        \"ext_identifiers\": pack_identifiers({\"bid\": bid}),\n",
    "        \"ext_notes\": \" | \".join(safe_list(full.get(\"note\"))) if full.get(\"note\") else \"\",\n",
    "        \"ext_url\": sbn_bid_to_page_url(bid)\n",
    "    }\n",
    "\n",
    "def from_ol(best: dict) -> dict:\n",
    "    d = best.get(\"raw\") or {}\n",
    "    identifiers = {}\n",
    "    for k in [\"isbn\", \"oclc\", \"lccn\"]:\n",
    "        if k in d and d[k]:\n",
    "            identifiers[k] = d[k][:10] if isinstance(d[k], list) else d[k]\n",
    "    key = best.get(\"key\",\"\") or d.get(\"key\",\"\")\n",
    "    return {\n",
    "        \"ext_source\": \"Open Library\",\n",
    "        \"ext_match_score\": f\"{best.get('score',0):.3f}\",\n",
    "        \"ext_title\": best.get(\"title\") or \"\",\n",
    "        \"ext_author\": best.get(\"author\") or \"\",\n",
    "        \"ext_year\": str(best.get(\"year\") or \"\"),\n",
    "        \"ext_publisher\": best.get(\"publisher\") or \"\",\n",
    "        \"ext_place\": best.get(\"place\") or \"\",\n",
    "        \"ext_language\": best.get(\"language\") or \"\",\n",
    "        \"ext_physical_desc\": \"\",\n",
    "        \"ext_identifiers\": pack_identifiers(identifiers),\n",
    "        \"ext_notes\": \"\",\n",
    "        \"ext_url\": f\"https://openlibrary.org{key}\" if key else \"\"\n",
    "    }\n",
    "\n",
    "def from_loc(best: dict) -> dict:\n",
    "    return {\n",
    "        \"ext_source\": \"Library of Congress\",\n",
    "        \"ext_match_score\": f\"{best.get('score',0):.3f}\",\n",
    "        \"ext_title\": best.get(\"title\") or \"\",\n",
    "        \"ext_author\": best.get(\"author\") or \"\",\n",
    "        \"ext_year\": str(extract_year(best.get(\"date\") or \"\") or \"\"),\n",
    "        \"ext_publisher\": \"\",\n",
    "        \"ext_place\": \"\",\n",
    "        \"ext_language\": \"\",\n",
    "        \"ext_physical_desc\": \"\",\n",
    "        \"ext_identifiers\": pack_identifiers({\"loc_id\": best.get(\"id\")}),\n",
    "        \"ext_notes\": \"\",\n",
    "        \"ext_url\": best.get(\"id\") or \"\"\n",
    "    }\n",
    "\n",
    "def from_ia(best: dict) -> dict:\n",
    "    full = best.get(\"full\") or {}\n",
    "    md = full.get(\"metadata\") or {}\n",
    "\n",
    "    def md1(k):\n",
    "        v = md.get(k, \"\")\n",
    "        if isinstance(v, list):\n",
    "            return v[0] if v else \"\"\n",
    "        return v or \"\"\n",
    "\n",
    "    ident = best.get(\"identifier\") or \"\"\n",
    "    return {\n",
    "        \"ext_source\": \"Internet Archive\",\n",
    "        \"ext_match_score\": f\"{best.get('score',0):.3f}\",\n",
    "        \"ext_title\": md1(\"title\") or best.get(\"title\") or \"\",\n",
    "        \"ext_author\": md1(\"creator\") or best.get(\"author\") or \"\",\n",
    "        \"ext_year\": str(extract_year(md1(\"date\")) or best.get(\"year\") or \"\"),\n",
    "        \"ext_publisher\": md1(\"publisher\"),\n",
    "        \"ext_place\": md1(\"publishplace\"),\n",
    "        \"ext_language\": md1(\"language\"),\n",
    "        \"ext_physical_desc\": md1(\"description\"),\n",
    "        \"ext_identifiers\": pack_identifiers({\"identifier\": ident}),\n",
    "        \"ext_notes\": md1(\"notes\"),\n",
    "        \"ext_url\": f\"https://archive.org/details/{ident}\" if ident else \"\"\n",
    "    }\n",
    "\n",
    "def choose_best_candidate(book: dict, pass_no: int):\n",
    "    threshold = MIN_SCORE_FOUND_1ST if pass_no == 1 else MIN_SCORE_FOUND_2ND\n",
    "\n",
    "    sbn = sbn_candidates(book, pass_no)\n",
    "    if sbn and sbn[0][\"score\"] >= threshold:\n",
    "        return sbn[0], from_sbn(sbn[0])\n",
    "\n",
    "    ol = ol_candidates(book, pass_no)\n",
    "    if ol and ol[0][\"score\"] >= threshold:\n",
    "        return ol[0], from_ol(ol[0])\n",
    "\n",
    "    loc = loc_candidates(book, pass_no)\n",
    "    if loc and loc[0][\"score\"] >= threshold:\n",
    "        return loc[0], from_loc(loc[0])\n",
    "\n",
    "    ia = ia_candidates(book, pass_no)\n",
    "    if ia and ia[0][\"score\"] >= threshold:\n",
    "        return ia[0], from_ia(ia[0])\n",
    "\n",
    "    return None, {\n",
    "        \"ext_source\":\"NOT_FOUND\",\"ext_match_score\":\"0.000\",\n",
    "        \"ext_title\":\"\",\"ext_author\":\"\",\"ext_year\":\"\",\n",
    "        \"ext_publisher\":\"\",\"ext_place\":\"\",\"ext_language\":\"\",\n",
    "        \"ext_physical_desc\":\"\",\"ext_identifiers\":pack_identifiers({}),\n",
    "        \"ext_notes\":\"\",\"ext_url\":\"\"\n",
    "    }\n",
    "\n",
    "# ---------------------------\n",
    "# RUN PASS 1 (resumable)\n",
    "# ---------------------------\n",
    "prog = load_progress()\n",
    "done = set(int(x) for x in prog.get(\"done_n_scheda\", []))\n",
    "\n",
    "if os.path.exists(OUTPUT_LONG):\n",
    "    out_df = pd.read_csv(OUTPUT_LONG)\n",
    "    out_df[\"n_scheda\"] = out_df[\"n_scheda\"].astype(int)\n",
    "    out_df[\"campo\"] = out_df[\"campo\"].astype(str)\n",
    "    out_df[\"valore\"] = out_df[\"valore\"].astype(str)\n",
    "else:\n",
    "    out_df = df.copy()\n",
    "\n",
    "already_ext = set()\n",
    "mask_ext = out_df[\"campo\"].astype(str).str.startswith(\"ext_\")\n",
    "for r in out_df.loc[mask_ext, [\"n_scheda\",\"campo\"]].itertuples(index=False):\n",
    "    already_ext.add((int(r.n_scheda), str(r.campo)))\n",
    "\n",
    "added_rows = []\n",
    "\n",
    "found_count = 0\n",
    "notfound_count = 0\n",
    "found_by_source = {\"OPAC SBN\": 0, \"Open Library\": 0, \"Library of Congress\": 0, \"Internet Archive\": 0}\n",
    "\n",
    "for book_number, (n_scheda, g) in enumerate(groups, start=1):\n",
    "    print(f\"[PASS1 {book_number}/{n_books}] n_scheda={n_scheda}\")\n",
    "    if int(n_scheda) in done:\n",
    "        continue\n",
    "\n",
    "    book = book_input_from_group(g)\n",
    "    _, ext = choose_best_candidate(book, pass_no=1)\n",
    "    src = ext.get(\"ext_source\")\n",
    "\n",
    "    if src != \"NOT_FOUND\":\n",
    "        found_count += 1\n",
    "        if src in found_by_source:\n",
    "            found_by_source[src] += 1\n",
    "    else:\n",
    "        notfound_count += 1\n",
    "\n",
    "    for k in STANDARD_FIELDS:\n",
    "        key = (int(n_scheda), k)\n",
    "        if key in already_ext:\n",
    "            continue\n",
    "        added_rows.append({\"n_scheda\": int(n_scheda), \"campo\": k, \"valore\": ext.get(k,\"\")})\n",
    "        already_ext.add(key)\n",
    "\n",
    "    done.add(int(n_scheda))\n",
    "    prog[\"done_n_scheda\"] = sorted(done)\n",
    "    prog[\"last_book_number\"] = book_number\n",
    "    save_progress(prog)\n",
    "\n",
    "    if book_number % 10 == 0:\n",
    "        if added_rows:\n",
    "            out_df = pd.concat([out_df, pd.DataFrame(added_rows)], ignore_index=True)\n",
    "            added_rows = []\n",
    "        out_df.to_csv(OUTPUT_LONG, index=False)\n",
    "        print(\n",
    "            f\"Checkpoint PASS1: {OUTPUT_LONG} | \"\n",
    "            f\"found={found_count} not_found={notfound_count} | \"\n",
    "            f\"SBN={found_by_source['OPAC SBN']} \"\n",
    "            f\"OL={found_by_source['Open Library']} \"\n",
    "            f\"LoC={found_by_source['Library of Congress']} \"\n",
    "            f\"IA={found_by_source['Internet Archive']}\"\n",
    "        )\n",
    "\n",
    "if added_rows:\n",
    "    out_df = pd.concat([out_df, pd.DataFrame(added_rows)], ignore_index=True)\n",
    "out_df.to_csv(OUTPUT_LONG, index=False)\n",
    "\n",
    "print(\n",
    "    f\"PASS1 completed. LONG: {OUTPUT_LONG} | \"\n",
    "    f\"found={found_count} not_found={notfound_count} | \"\n",
    "    f\"SBN={found_by_source['OPAC SBN']} \"\n",
    "    f\"OL={found_by_source['Open Library']} \"\n",
    "    f\"LoC={found_by_source['Library of Congress']} \"\n",
    "    f\"IA={found_by_source['Internet Archive']}\"\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# PASS 2: ONLY NOT_FOUND (overwrite ext_* for those)\n",
    "# ---------------------------\n",
    "# Identify NOT_FOUND from the CURRENT long\n",
    "df_ext = out_df[out_df[\"campo\"]==\"ext_source\"][[\"n_scheda\",\"valore\"]].copy()\n",
    "df_ext[\"n_scheda\"] = df_ext[\"n_scheda\"].astype(int)\n",
    "not_found_ids = set(df_ext.loc[df_ext[\"valore\"]==\"NOT_FOUND\", \"n_scheda\"].tolist())\n",
    "\n",
    "print(f\"PASS2 starting: NOT_FOUND records = {len(not_found_ids)}\")\n",
    "\n",
    "# Build a quick lookup from original input groups\n",
    "group_map = {int(n): g for (n, g) in groups}\n",
    "\n",
    "# Remove existing ext_* rows for NOT_FOUND (so we can rewrite them cleanly)\n",
    "mask_drop = out_df[\"n_scheda\"].isin(list(not_found_ids)) & out_df[\"campo\"].astype(str).str.startswith(\"ext_\")\n",
    "out_df = out_df.loc[~mask_drop].copy()\n",
    "\n",
    "# Rebuild already_ext after drop\n",
    "already_ext = set()\n",
    "mask_ext = out_df[\"campo\"].astype(str).str.startswith(\"ext_\")\n",
    "for r in out_df.loc[mask_ext, [\"n_scheda\",\"campo\"]].itertuples(index=False):\n",
    "    already_ext.add((int(r.n_scheda), str(r.campo)))\n",
    "\n",
    "added_rows = []\n",
    "\n",
    "pass2_found = 0\n",
    "pass2_by_source = {\"OPAC SBN\": 0, \"Open Library\": 0, \"Library of Congress\": 0, \"Internet Archive\": 0}\n",
    "pass2_still_not_found = 0\n",
    "\n",
    "for idx, n_scheda in enumerate(sorted(not_found_ids), start=1):\n",
    "    g = group_map.get(int(n_scheda))\n",
    "    if g is None:\n",
    "        continue\n",
    "    print(f\"[PASS2 {idx}/{len(not_found_ids)}] n_scheda={n_scheda}\")\n",
    "\n",
    "    book = book_input_from_group(g)\n",
    "    _, ext = choose_best_candidate(book, pass_no=2)\n",
    "    src = ext.get(\"ext_source\")\n",
    "\n",
    "    if src != \"NOT_FOUND\":\n",
    "        pass2_found += 1\n",
    "        if src in pass2_by_source:\n",
    "            pass2_by_source[src] += 1\n",
    "    else:\n",
    "        pass2_still_not_found += 1\n",
    "\n",
    "    for k in STANDARD_FIELDS:\n",
    "        key = (int(n_scheda), k)\n",
    "        if key in already_ext:\n",
    "            continue\n",
    "        added_rows.append({\"n_scheda\": int(n_scheda), \"campo\": k, \"valore\": ext.get(k,\"\")})\n",
    "        already_ext.add(key)\n",
    "\n",
    "    if idx % 10 == 0:\n",
    "        if added_rows:\n",
    "            out_df = pd.concat([out_df, pd.DataFrame(added_rows)], ignore_index=True)\n",
    "            added_rows = []\n",
    "        out_df.to_csv(OUTPUT_LONG, index=False)\n",
    "        print(\n",
    "            f\"Checkpoint PASS2: {OUTPUT_LONG} | \"\n",
    "            f\"pass2_found={pass2_found} pass2_still_not_found={pass2_still_not_found} | \"\n",
    "            f\"SBN={pass2_by_source['OPAC SBN']} \"\n",
    "            f\"OL={pass2_by_source['Open Library']} \"\n",
    "            f\"LoC={pass2_by_source['Library of Congress']} \"\n",
    "            f\"IA={pass2_by_source['Internet Archive']}\"\n",
    "        )\n",
    "\n",
    "if added_rows:\n",
    "    out_df = pd.concat([out_df, pd.DataFrame(added_rows)], ignore_index=True)\n",
    "\n",
    "out_df.to_csv(OUTPUT_LONG, index=False)\n",
    "\n",
    "print(\n",
    "    f\"PASS2 completed. LONG: {OUTPUT_LONG} | \"\n",
    "    f\"pass2_found={pass2_found} pass2_still_not_found={pass2_still_not_found} | \"\n",
    "    f\"SBN={pass2_by_source['OPAC SBN']} \"\n",
    "    f\"OL={pass2_by_source['Open Library']} \"\n",
    "    f\"LoC={pass2_by_source['Library of Congress']} \"\n",
    "    f\"IA={pass2_by_source['Internet Archive']}\"\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# LONG -> WIDE\n",
    "# ---------------------------\n",
    "df_long = out_df.copy()\n",
    "df_long[\"valore\"] = df_long[\"valore\"].fillna(\"\").astype(str)\n",
    "df_long.loc[df_long[\"campo\"] == \"n_scheda\", \"campo\"] = \"n_scheda_field\"\n",
    "\n",
    "agg = (df_long\n",
    "       .groupby([\"n_scheda\",\"campo\"])[\"valore\"]\n",
    "       .agg(lambda s: \" | \".join([v.strip() for v in s if isinstance(v,str) and v.strip()]))\n",
    "       .reset_index())\n",
    "\n",
    "df_wide = agg.pivot(index=\"n_scheda\", columns=\"campo\", values=\"valore\").reset_index()\n",
    "\n",
    "front = [\"n_scheda\",\"titolo\",\"autore\",\"data_pub\",\"luogo_pub\",\"editore\",\"collezion\",\n",
    "         \"ext_source\",\"ext_match_score\",\"ext_title\",\"ext_author\",\"ext_year\",\"ext_url\",\"ext_identifiers\",\"n_scheda_field\"]\n",
    "cols = [c for c in front if c in df_wide.columns] + [c for c in df_wide.columns if c not in front]\n",
    "df_wide = df_wide[cols]\n",
    "\n",
    "df_wide.to_csv(OUTPUT_WIDE, index=False)\n",
    "print(f\"WIDE creato: {OUTPUT_WIDE} | righe={len(df_wide)} colonne={len(df_wide.columns)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
